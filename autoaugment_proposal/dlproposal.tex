\documentclass[10pt,twocolumn,letterpaper]{article}

%\usepackage[showframe]{geometry}
\usepackage{geometry}
\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{soul}

\setlength{\voffset}{-50pt}
%\setlength{\hoffset}{-25pt}
%!TEX encoding = UTF-8 Unicode\setlength{\headsep}{2pt}
\setlength{\textheight}{715pt}
%\setlength{\textwidth}{511pt}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
% \setcounter{page}{1}
\begin{document}

%%%%%%%%% TITLE
\title{Project Proposal: An Alternative Algorithm for Learning Augmentation Policies from Data and the Generalization on other Datasets}

\author{
    	\small{NAMES: Samuel Frommenwiler, Gian K\"onig, Colin K\"alin} \\
   	\small{NETHZ: fsamuel, koenigg, ckaelin}\\
	\small{EMAIL: \{fsamuel, koenigg, ckaelin\}$@$student.ethz.ch}\\
    	\small{ID: XX-XXX-XXX, 09-913-245, 14-935-118}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
   This document states the proposed content of the project for the deep learning course at ETH as required in~\cite{DL18}. The goal is to investigate Baysien Optimization to tune hyperparameters for automated data-augmentation policies.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{AutoAugment with Reinforcement Learning}
When the dataset is limited, a procedure called data augmentation can be used to increase both the amount and the diversity of the data randomly. For a dataset of natural images like the CIFAR-10, common dataset augmentation methods are random cropping, image mirroring, color shifting and color whitening. These methods require expert knowledge and time. Therefore an automated approach was introduced in order to find the best policies~\cite{Ekin}. Cubuk et. al. present a process of finding an efficient data augmentation policy, in which each policy contains possible augmentation operations. Each operation contains an image processing function (e. g. translation, rotation or color normalization) combined with a probability that this function is applied with a corresponding magnitude. To find the best choices of these functions and suitable scaling factors, Ekin et. al. use a Reinforcement Learning search algorithm such that a neural network, trained on these hyper parameters, yields the best validation accuracy.


\section{AutoAugment with Baysien Optimization}
\hl{which net? WideResNet only?}\newline
We intend to investigate the Bayesian Optimization \cite{2018arXiv180702811F},~\cite{Goodfellow-et-al-2016} approach with a Tree Parzen Esimator~\cite{Kaggle_AMT} by using the Hyperopt library~\cite{HyperOpt} with the help of~\cite{BO_Hyperopt}. For this specific task,~\cite{2017arXiv171010564T} gives us a guideline on how to integrate Baysien Optimization into the data augmentation task. The questions we would like to answer are: Can we beat the $2.68\%$ barrier on CIFAR-10 with a Baysien approach? Or what are the issues with the proposed approach if this baseline can not be achieved?
We want to investigate the problem with two different approach. The first is based on the code provided by Ekin et. al. The second approach is to build up another classifier model as described in~\cite{cifar-10-cl}, integrate Baysien Optimization and compare the results to the first approach.

We started to investigate the code and already run a few test runs with with the CIFAR-10 dataset and the Wide-ResNet~\cite{Ekin}. On a MacBook Pro, one epoch takes around 5.5 hours. On the Google Cloud~\cite{GCloud} using GPUs we were able to run the 200 epochs in 9h15min.

% https://towardsdatascience.com/cifar-10-image-classification-in-tensorflow-5b501f7dc77c
% https://github.com/deep-diver/CIFAR10-img-classification-tensorflow

% https://towardsdatascience.com/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a
% https://towardsdatascience.com/an-introductory-example-of-bayesian-optimization-in-python-with-hyperopt-aae40fff4ff0
% https://www.kaggle.com/willkoehrsen/automated-model-tuning
% https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering
% https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering-p2
% https://arxiv.org/pdf/1710.10564.pdf


{\small
\bibliographystyle{ieee}
\bibliography{dlbib}
}

\end{document}
